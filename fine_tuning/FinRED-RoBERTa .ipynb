{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e4cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from os.path import exists\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab58117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 형식: (스페셜 토큰 들어간 문장, 라벨)\n",
    "#데이터 처리:\n",
    "#sent 문장에서 Pointer를 이용해 문장에 Marked 토큰 추가\n",
    "#모델 Interence 후 , 각 Class 에 대해 sigmoid 로 스코어 측정. Ground truth는 Pointer에게 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53587cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device is:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 1#\n",
    "\n",
    "device = (torch.device(f'cuda:{GPU_NUM}') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "torch.cuda.set_device(device)\n",
    "#device = torch.device('cpu')#cpu\n",
    "\n",
    "print(\"Current Device is: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2edab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FOLDER = './dataset/relation_extraction/FinRED/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098a8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_FOLDER + 'relations.txt') as relationfile:\n",
    "    RELATION = relationfile.read()\n",
    "RELATION = RELATION.replace(' ', '_').replace('product/material_produced','product_or_material_produced').replace('director/manager','director_/_manager').split('\\n')\n",
    "RELATION = {k: i for i, k in enumerate(RELATION)}\n",
    "SPECIAL_TOKENS = ['<Sub>', '</Sub>', '<Obj>', '</Obj>']#index 50265~50268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0617e6e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_or_material_produced': 0,\n",
       " 'manufacturer': 1,\n",
       " 'distributed_by': 2,\n",
       " 'industry': 3,\n",
       " 'position_held': 4,\n",
       " 'original_broadcaster': 5,\n",
       " 'owned_by': 6,\n",
       " 'founded_by': 7,\n",
       " 'distribution_format': 8,\n",
       " 'headquarters_location': 9,\n",
       " 'stock_exchange': 10,\n",
       " 'currency': 11,\n",
       " 'parent_organization': 12,\n",
       " 'chief_executive_officer': 13,\n",
       " 'director_/_manager': 14,\n",
       " 'owner_of': 15,\n",
       " 'operator': 16,\n",
       " 'member_of': 17,\n",
       " 'employer': 18,\n",
       " 'chairperson': 19,\n",
       " 'platform': 20,\n",
       " 'subsidiary': 21,\n",
       " 'legal_form': 22,\n",
       " 'publisher': 23,\n",
       " 'developer': 24,\n",
       " 'brand': 25,\n",
       " 'business_division': 26,\n",
       " 'location_of_formation': 27,\n",
       " 'creator': 28,\n",
       " '': 29}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d80b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinRED(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        #transforms.Compose([transforms.ToTensor(),lambda x: x.repeat(1,3,1,1), lambda x: torch.mul(x,torch.rand(3,28,28))])\n",
    "        self.train = train\n",
    "        self.data = []\n",
    "        if self.train == True:\n",
    "            setname = 'train'\n",
    "        else:\n",
    "            setname = 'test'\n",
    "\n",
    "        with open(PATH_FOLDER + setname + '.pointer') as pointer_file:\n",
    "            pointer = pointer_file.read().strip()\n",
    "            pointer = pointer.split('\\n')\n",
    "            \n",
    "        with open(PATH_FOLDER + setname + '.sent') as sent_file:\n",
    "            sent = sent_file.read().strip()\n",
    "            sent = sent.split('\\n')\n",
    "        lengths = list(map(lambda x: len(x), sent))\n",
    "        print(len(sent))\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length>1000:\n",
    "                pointer[i] = \"todel\"\n",
    "                sent [i] =\"todel\"\n",
    "        pointer = [i for i in pointer if i !=\"todel\"]\n",
    "        sent = [i for i in sent if i !=\"todel\"]\n",
    "        print(len(sent))\n",
    "        for a, b  in zip(sent, pointer):#한 sentence에 대해\n",
    "            tuples = list(map(lambda x: x.strip(), b.split('|')))\n",
    "            tuple_dict = {}\n",
    "            tuple_keys = list(map(lambda x: ' '.join(x.split(' ')[:4]), tuples))\n",
    "            tuple_values = list(map(lambda x: x.split(' ')[4], tuples))\n",
    "            for tuple_, category in zip(tuple_keys, tuple_values):\n",
    "                if tuple_ not in tuple_dict:\n",
    "                    tuple_dict[tuple_] = [category]\n",
    "                else:\n",
    "                    tuple_dict[tuple_] += [category]\n",
    "            for tuple_, category in tuple_dict.items(): #튜플 하나\n",
    "                sent_splitted = a.split(' ')\n",
    "                splitted = tuple_.split(' ')\n",
    "                token_index = list(map(int,splitted))\n",
    "                token_index = [x+y for x,y in zip(token_index, [0,1,0,1])]#공간 마련\n",
    "                index_map = zip(token_index,SPECIAL_TOKENS)\n",
    "                index_map = sorted(index_map, reverse =True)\n",
    "                for index, token in index_map:\n",
    "                    sent_splitted.insert(index, token)\n",
    "                input_string = ' '.join(sent_splitted)\n",
    "                label = list(map(lambda x: RELATION[x],category))#list of indexes\n",
    "                one_hot = torch.zeros(len(RELATION))\n",
    "                one_hot[[label]] = 1\n",
    "                self.data.append((input_string, one_hot))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][0] , self.data[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cee9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinRED_dev(Dataset):\n",
    "    def __init__(self, train = True):\n",
    "        #transforms.Compose([transforms.ToTensor(),lambda x: x.repeat(1,3,1,1), lambda x: torch.mul(x,torch.rand(3,28,28))])\n",
    "        self.train = train\n",
    "        self.data = []\n",
    "        if self.train == True:\n",
    "            setname = 'dev'\n",
    "        else:\n",
    "            setname = 'dev'\n",
    "\n",
    "        with open(PATH_FOLDER + setname + '.pointer') as pointer_file:\n",
    "            pointer = pointer_file.read().strip()\n",
    "            pointer = pointer.split('\\n')\n",
    "            \n",
    "        with open(PATH_FOLDER + setname + '.sent') as sent_file:\n",
    "            sent = sent_file.read().strip()\n",
    "            sent = sent.split('\\n')\n",
    "        lengths = list(map(lambda x: len(x), sent))\n",
    "        print(len(sent))\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length>1000:\n",
    "                pointer[i] = \"todel\"\n",
    "                sent [i] =\"todel\"\n",
    "        pointer = [i for i in pointer if i !=\"todel\"]\n",
    "        sent = [i for i in sent if i !=\"todel\"]\n",
    "        print(len(sent))\n",
    "        for a, b  in zip(sent, pointer):#한 sentence에 대해\n",
    "            tuples = list(map(lambda x: x.strip(), b.split('|')))\n",
    "            tuple_dict = {}\n",
    "            tuple_keys = list(map(lambda x: ' '.join(x.split(' ')[:4]), tuples))\n",
    "            tuple_values = list(map(lambda x: x.split(' ')[4], tuples))\n",
    "            for tuple_, category in zip(tuple_keys, tuple_values):\n",
    "                if tuple_ not in tuple_dict:\n",
    "                    tuple_dict[tuple_] = [category]\n",
    "                else:\n",
    "                    tuple_dict[tuple_] += [category]\n",
    "            for tuple_, category in tuple_dict.items(): #튜플 하나\n",
    "                sent_splitted = a.split(' ')\n",
    "                splitted = tuple_.split(' ')\n",
    "                token_index = list(map(int,splitted))\n",
    "                token_index = [x+y for x,y in zip(token_index, [0,1,0,1])]#공간 마련\n",
    "                index_map = zip(token_index,SPECIAL_TOKENS)\n",
    "                index_map = sorted(index_map, reverse =True)\n",
    "                for index, token in index_map:\n",
    "                    sent_splitted.insert(index, token)\n",
    "                input_string = ' '.join(sent_splitted)\n",
    "                label = list(map(lambda x: RELATION[x],category))#list of indexes\n",
    "                one_hot = torch.zeros(len(RELATION))\n",
    "                one_hot[[label]] = 1\n",
    "                self.data.append((input_string, one_hot))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][0] , self.data[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d0341dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RE, self).__init__()\n",
    "        self.modelname = \"FinRED ver 1.0\"\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        self.pretrained = RobertaModel.from_pretrained('roberta-base').to(device=device)\n",
    "        #==========토큰 추가===========================\n",
    "        special_tokens_dict = {'additional_special_tokens': SPECIAL_TOKENS}\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        self.pretrained.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.header = nn.Linear(768*2,len(RELATION)).to(device=device)\n",
    "    def forward(self, x):#x: 문제 input\n",
    "        batchsize = len(x)\n",
    "        encoded_input = self.tokenizer(x, return_tensors='pt', padding='max_length', truncation=True).to(device=device)\n",
    "        nongpu_input = self.tokenizer(x)['input_ids']\n",
    "        start_list = [list(map(lambda x: x.index(50265),nongpu_input)), list(map(lambda x: x.index(50267),nongpu_input))]\n",
    "        #start_list = [list(map(lambda x: x.index(50265),encoded_input['input_ids'])), list(map(lambda x: x.index(50267),encoded_input['input_ids']))]\n",
    "        output = self.pretrained(**encoded_input)[0]#batchsize*sequence maximum size*embedding\n",
    "        input_linear = torch.cat((output[range(batchsize),start_list[0]], output[range(batchsize),start_list[1]]), dim = -1)\n",
    "        final_output = self.header(input_linear)\n",
    "        return final_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d44051c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e565ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(optimizer, model, loss_fn, train_loader, test_loader, epochs, mini_per_batch):\n",
    "    for epoch in range(1,epochs+1):\n",
    "        model.train()\n",
    "        loss_train_epoch = 0.0\n",
    "        loss_batch = 0.0\n",
    "        mb_passed = 0\n",
    "        optimizer.zero_grad()#매 epoch 시작마다 Grad 초기화\n",
    "        for sentences, labels in train_loader:\n",
    "            labels= labels.to(device=device)\n",
    "            outputs = model(sentences)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            loss_train_epoch += loss.item()\n",
    "            loss_batch += loss.item()\n",
    "            mb_passed += 1\n",
    "            if mb_passed%mini_per_batch == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_batch=loss_batch/mini_per_batch\n",
    "                print(\"epoch:\",epoch, \" minibatch train_loss: \", loss_batch)\n",
    "                loss_batch = 0.0\n",
    "                mb_passed = 0\n",
    "        loss_train_epoch = loss_train_epoch/len(train_loader)\n",
    "        loss_validation, _, _, F1 = evaluate_model(model, test_loader, loss_fn)\n",
    "        print(\"epoch:\",epoch, \" train_loss: \", loss_train_epoch, \" val_loss: \", loss_validation, \" val_F1: \", F1)\n",
    "        torch.save(model.state_dict(), model.modelname+'.pt')#debug\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "def accuracy_metrics(tensor, target):\n",
    "    tensor = tensor == 1\n",
    "    #print(torch.sum(tensor))\n",
    "    target = target == 1\n",
    "    logical_not = torch.logical_not\n",
    "    def andsum(tensor1, tensor2):\n",
    "        return torch.sum(torch.logical_and(tensor1,tensor2)).item()\n",
    "    TP = andsum(tensor, target)\n",
    "    TN = andsum(logical_not(tensor), logical_not(target))\n",
    "    FP = andsum(tensor, logical_not(target))\n",
    "    FN = andsum(logical_not(tensor), target)\n",
    "\n",
    "    return torch.tensor([TP, TN, FP, FN], dtype = torch.long)\n",
    "        \n",
    "def evaluate_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    accus = torch.zeros(4, dtype = torch.long)#TP,PN,FP,FN\n",
    "    loss_test = 0.0\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in data_loader:\n",
    "            labels = labels.to(device=device)#batchsize*30 one_hot\n",
    "            outputs = model(sentences)\n",
    "            output_labels = (sigmoid(outputs)>0.5).long()\n",
    "            accus += accuracy_metrics(output_labels, labels.long())\n",
    "            #print(accus)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss_test += loss.item()\n",
    "    loss_test = loss_test/len(data_loader)\n",
    "    precision = accus[0]/(accus[0]+accus[2])#TP/(TP+FP)\n",
    "    recall = accus[0]/(accus[0]+accus[3])#TP/(TP+FN)\n",
    "    F1 = 2*(precision*recall)/(precision+recall)\n",
    "    model.train()\n",
    "    return loss_test, precision, recall,  F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71491748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE(\n",
      "  (pretrained): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50269, 768)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (header): Linear(in_features=1536, out_features=30, bias=True)\n",
      ")\n",
      "FinRED ver 1.0.pt exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "5700\n",
      "5647\n",
      "1068\n",
      "1060\n",
      "epoch: 1  minibatch train_loss:  0.1428983944811319\n",
      "epoch: 1  minibatch train_loss:  0.14236399521561047\n"
     ]
    }
   ],
   "source": [
    "EPOCHES = 100\n",
    "BATCHSIZE = 1900 #주의: BATCHSIZE_MINI의 배수로 설정하시오\n",
    "BATCHSIZE_MINI = 25\n",
    "MINI_PER_BATCH = int(BATCHSIZE/BATCHSIZE_MINI)\n",
    "LR = 4e-6\n",
    "WEIGHT_DECAY = 4e-5\n",
    "\n",
    "print(model)\n",
    "filename_parameters = model.modelname+'.pt'\n",
    "if exists(filename_parameters):\n",
    "    print(filename_parameters, \"exists.\")\n",
    "    print(model.load_state_dict(torch.load(\"./\" + filename_parameters)))\n",
    "else:\n",
    "    print(filename_parameters, \"does not exists.\")\n",
    "    \n",
    "train_loader = DataLoader(FinRED(train=True), batch_size = BATCHSIZE_MINI, shuffle = True)\n",
    "test_loader = DataLoader(FinRED(train=False), batch_size = 20, shuffle =True)\n",
    "\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss()#reduction:mean. reduction= None 일 경우에는 주어진 batchsize짜리 vector\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = WEIGHT_DECAY)\n",
    "training_loop(optimizer, model, loss_fn, train_loader, test_loader, EPOCHES, MINI_PER_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e741bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(model, test_loader, loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcd0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_loader = DataLoader(FinRED_dev(train=False), batch_size = 20, shuffle =True)\n",
    "print(evaluate_model(model, dev_loader, loss_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf3e08",
   "metadata": {},
   "source": [
    "epochs:13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ec0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [i for i in model.header.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce17548",
   "metadata": {},
   "outputs": [],
   "source": [
    "pse = [i for i in model.pretrained.embeddings.word_embeddings.parameters()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a2b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f05b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47837c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
